{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.27.0\n!pip install mlflow","metadata":{"id":"ZsBCQDT8I5ki","execution":{"iopub.status.busy":"2023-05-16T21:38:15.062805Z","iopub.execute_input":"2023-05-16T21:38:15.063302Z","iopub.status.idle":"2023-05-16T21:38:54.453153Z","shell.execute_reply.started":"2023-05-16T21:38:15.063272Z","shell.execute_reply":"2023-05-16T21:38:54.451836Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device =\"cuda:0\"","metadata":{"id":"HDUsOeGlYz1P","execution":{"iopub.status.busy":"2023-05-16T21:38:54.457279Z","iopub.execute_input":"2023-05-16T21:38:54.457776Z","iopub.status.idle":"2023-05-16T21:38:54.465069Z","shell.execute_reply.started":"2023-05-16T21:38:54.457732Z","shell.execute_reply":"2023-05-16T21:38:54.463879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPModel, CLIPProcessor\n\n\nclass CLIPFeatureExtractor:\n    def __init__(self):\n        model_name = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n        self.model = CLIPModel.from_pretrained(model_name)\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model.to(self.device)\n\n    @torch.no_grad()\n    def get_text_features(self, text):\n        inputs = self.processor(text=text, return_tensors=\"pt\")\n        inputs = inputs.to(self.device)\n        text_features = self.model.get_text_features(**inputs)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n        text_features = text_features.tolist()\n        return text_features\n\n    @torch.no_grad()\n    def get_image_features(self, images):\n        inputs = self.processor(images=images, return_tensors=\"pt\")\n        inputs = inputs.to(self.device)\n        image_features = self.model.get_image_features(**inputs)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        image_features = image_features.detach().cpu().numpy()\n        return image_features\n\nfrom PIL import Image\nimport requests\n\nprocessor = CLIPFeatureExtractor()","metadata":{"id":"l5UUna6GQJ7_","outputId":"da4bee6d-6806-40ff-f801-d6392874052a","execution":{"iopub.status.busy":"2023-05-16T21:38:54.466654Z","iopub.execute_input":"2023-05-16T21:38:54.467599Z","iopub.status.idle":"2023-05-16T21:39:50.312950Z","shell.execute_reply.started":"2023-05-16T21:38:54.467563Z","shell.execute_reply":"2023-05-16T21:39:50.311794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.exceptions import UndefinedMetricWarning\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\nimport pandas as pd\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\ntransform_augment_train = transforms.Compose(\n    [transforms.Resize((230,230)),\n        transforms.RandomApply([transforms.RandomRotation(30,),],p=0.6),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomErasing(p=0.3,scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n    ])\n\ntransform = transforms.Compose(\n    [transforms.Resize((224,224)),     \n    ])\n\n","metadata":{"id":"FzMznN8QSdnO","execution":{"iopub.status.busy":"2023-05-16T21:39:50.316429Z","iopub.execute_input":"2023-05-16T21:39:50.317282Z","iopub.status.idle":"2023-05-16T21:39:51.530666Z","shell.execute_reply.started":"2023-05-16T21:39:50.317242Z","shell.execute_reply":"2023-05-16T21:39:51.529622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2class = [\"land slide\",\"drought\",\"urban fire\",\"infrastructure\",\"flooding\",\"earthquake\",\"wild fire\"]\n\npath2folder = [\"/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Land_Disaster/Land_Slide\",\n              \"/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Land_Disaster/Drought\",\n               \"/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Fire_Disaster/Urban_Fire\",\n               \"/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Damaged_Infrastructure/Infrastructure\",\n               \"/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Water_Disaster\",\n               \"/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Damaged_Infrastructure/Earthquake\",\n               \"/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Fire_Disaster/Wild_Fire\",\n              ]","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:39:51.532530Z","iopub.execute_input":"2023-05-16T21:39:51.533276Z","iopub.status.idle":"2023-05-16T21:39:51.539440Z","shell.execute_reply.started":"2023-05-16T21:39:51.533240Z","shell.execute_reply":"2023-05-16T21:39:51.538456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mlflow import MlflowClient\n\nimport os \nos.environ['GOOGLE_APPLICATION_CREDENTIALS']='/kaggle/input/datasci-key-storage/krian-mai-krian-proj-386109-169baed358e1.json'\n\n\nclient = MlflowClient(\"http://34.142.181.201:5000\", \"http://34.142.181.201:5000\")\nprint(\"Running mlflow_tracking.py\")\n\nexperiment_id = \"1\"\n","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:39:51.541128Z","iopub.execute_input":"2023-05-16T21:39:51.542008Z","iopub.status.idle":"2023-05-16T21:39:54.817048Z","shell.execute_reply.started":"2023-05-16T21:39:51.541971Z","shell.execute_reply":"2023-05-16T21:39:54.815966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = []\nlabels = []\nfrom glob import glob\nfor i in range(len(id2class)):\n    paths = glob(path2folder[i]+\"/*\")\n    image_path+=paths\n    labels+=[i]*len(paths)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:39:54.820869Z","iopub.execute_input":"2023-05-16T21:39:54.821653Z","iopub.status.idle":"2023-05-16T21:39:55.216538Z","shell.execute_reply.started":"2023-05-16T21:39:54.821615Z","shell.execute_reply":"2023-05-16T21:39:55.215486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random \nfrom tqdm import tqdm\nrandom.seed(0)\n\ndf= pd.DataFrame({\"filename\":image_path,\"class\":labels})","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:39:55.218915Z","iopub.execute_input":"2023-05-16T21:39:55.219514Z","iopub.status.idle":"2023-05-16T21:39:55.235774Z","shell.execute_reply.started":"2023-05-16T21:39:55.219468Z","shell.execute_reply":"2023-05-16T21:39:55.234693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX=df[\"filename\"].tolist()\ny=df[\"class\"].tolist()\n\ntrain_images_path, val_images_path, train_labels, val_labels = train_test_split(X, y, test_size=0.1, random_state=0,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:39:55.238002Z","iopub.execute_input":"2023-05-16T21:39:55.238730Z","iopub.status.idle":"2023-05-16T21:39:55.394112Z","shell.execute_reply.started":"2023-05-16T21:39:55.238674Z","shell.execute_reply":"2023-05-16T21:39:55.393046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"err_idx = [131,\n 196,\n 370,\n 872,\n 1088,\n 1260,\n 1830,\n 2830,\n 2842,\n 3210,\n 3415,\n 4541,\n 4608,\n 4770,\n 4786,\n 4797,\n 4991,\n 5065,\n 5118,\n 5144,\n 5245,\n 5446,\n 5496,\n 5651,\n 5933,\n 5938,\n 5988,\n 6254,\n 6330,\n 6369,\n 6488,\n 6606,\n 6742,\n 7062,\n 7750,\n 7878,\n 7973,\n 7978,\n 7979,\n 7988,\n 8187,\n 8196,\n 8211,\n 8215,\n 8266,\n 8270,\n 8454,\n 8459,\n 8476,\n 8605]","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:39:55.397290Z","iopub.execute_input":"2023-05-16T21:39:55.397647Z","iopub.status.idle":"2023-05-16T21:39:55.410003Z","shell.execute_reply.started":"2023-05-16T21:39:55.397617Z","shell.execute_reply":"2023-05-16T21:39:55.408777Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nnp.random.seed(0)\nprint(len(train_images_path),len(train_labels))\nexpected_number = 1500\n\ndf = pd.DataFrame({\"path\":train_images_path,\"class\":train_labels})\n\nclass_number_add = dict(expected_number-df[\"class\"].value_counts())\ndf =df.groupby(\"class\",as_index=False).agg(list)\nfor i in class_number_add:\n    if class_number_add[i]>0:\n        train_images_path+=list(np.random.choice(df[\"path\"][i],class_number_add[i]))\n        train_labels+=[i for idx in range(class_number_add[i])]\nprint(len(train_images_path),len(train_labels))\nprint(pd.Series(train_labels).value_counts())\n\nfrom tqdm import tqdm\nfrom torchvision.io import read_image , ImageReadMode\n\nclass FondueDataset(Dataset):\n    \n    def __init__(self, \n                 images_path,\n                 labels, \n                 transforms=None,\n                 augment_transform=None):\n        \n        super().__init__()\n        self.input_dataset=[]\n        self.transforms = transforms\n        self.augment_transforms = augment_transform\n        idx = 0\n        for path,label in tqdm(zip(images_path,labels)):\n            if idx not in err_idx:\n                self.input_dataset.append([path,label])\n            idx+=1\n        \n            \n    def __len__(self):\n        return len(self.input_dataset)\n\n    def __getitem__(self, idx): \n        # img = Image.open(self.input_dataset[idx][0]).convert('RGB')\n        img = read_image(self.input_dataset[idx][0],ImageReadMode.RGB)/255\n        if not self.augment_transforms:\n            x = self.transforms(img)\n        else:\n            x= self.augment_transforms(img)\n        y = self.input_dataset[idx][1]\n        return torch.clamp(x,min=0.0,max=1.0),y\n\ntrainset = FondueDataset(train_images_path,train_labels,transform_augment_train)\nvalset = FondueDataset(val_images_path,val_labels,transform)","metadata":{"id":"DFj6tRfRu0rv","outputId":"7d8b8178-6ba9-4da4-d1c6-e9a4607438a5","execution":{"iopub.status.busy":"2023-05-16T21:40:36.555195Z","iopub.execute_input":"2023-05-16T21:40:36.562889Z","iopub.status.idle":"2023-05-16T21:40:36.799239Z","shell.execute_reply.started":"2023-05-16T21:40:36.562843Z","shell.execute_reply":"2023-05-16T21:40:36.793310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndef get_features(dataset):\n    all_features = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(DataLoader(dataset, batch_size=256)):\n            \n            features = processor.get_image_features(images)\n            all_features.append(torch.Tensor(features))\n            all_labels.append(labels)\n    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n\n# Calculate the image features\ntrain_features, train_labels = get_features(trainset)\ntest_features, test_labels = get_features(valset)\n","metadata":{"id":"A63AX5J1H60t","outputId":"cf6be89b-5cee-4293-e990-ba8d10d92db6","execution":{"iopub.status.busy":"2023-05-16T21:40:41.977227Z","iopub.execute_input":"2023-05-16T21:40:41.977633Z","iopub.status.idle":"2023-05-16T21:52:12.885571Z","shell.execute_reply.started":"2023-05-16T21:40:41.977595Z","shell.execute_reply":"2023-05-16T21:52:12.884415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(train_features,\"train_feat.pt\")\ntorch.save(test_features,\"test_feat.pt\")\ntorch.save(train_labels,\"train_labels.pt\")\ntorch.save(test_labels,\"test_labels.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-05-16T22:10:00.121996Z","iopub.execute_input":"2023-05-16T22:10:00.122392Z","iopub.status.idle":"2023-05-16T22:10:00.809991Z","shell.execute_reply.started":"2023-05-16T22:10:00.122361Z","shell.execute_reply":"2023-05-16T22:10:00.808896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.001,0.01,0.1,0.5,1,5,10]}\nclassifier = LogisticRegression(random_state=0, max_iter=1000)\ngrid_search = GridSearchCV(classifier, param_grid, cv=5)\n\ngrid_search.fit(train_features, train_labels)\n\nprint(\"Best Parameters: \", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:59:21.338212Z","iopub.execute_input":"2023-05-16T21:59:21.338755Z","iopub.status.idle":"2023-05-16T22:02:56.558079Z","shell.execute_reply.started":"2023-05-16T21:59:21.338671Z","shell.execute_reply":"2023-05-16T22:02:56.552606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nclassifier = LogisticRegression(random_state=0, C=10,max_iter=1000)\n\nclassifier.fit(train_features, train_labels)\nval_predictions = classifier.predict(test_features)\nf1=classification_report(test_labels,val_predictions,output_dict=True)[\"macro avg\"][\"f1-score\"]\n","metadata":{"id":"7Ra7unI6H60u","outputId":"ffe8be92-105f-4e62-9132-00e242ce2a65","execution":{"iopub.status.busy":"2023-05-16T22:03:20.842990Z","iopub.execute_input":"2023-05-16T22:03:20.843730Z","iopub.status.idle":"2023-05-16T22:03:37.592300Z","shell.execute_reply.started":"2023-05-16T22:03:20.843673Z","shell.execute_reply":"2023-05-16T22:03:37.590659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = client.create_run(experiment_id,run_name=\"DisasterClassifierLR\")\nprint(run.info.run_id)\nclient.log_param(run.info.run_id, key=\"C\", value=grid_search.best_params_[\"C\"])\nclient.log_metric(run.info.run_id, \"macro-f1\", f1)\n\nif not os.path.exists(\"model\"):\n    os.makedirs(\"model\")\n\nimport pickle \nfilename = '/kaggle/working/model/model.sav'\npickle.dump(classifier, open(filename, 'wb'))\n\nlocal_artifacts_path = \"/kaggle/working/model\"\nremote_artifacts_path = \"model\"\nclient.log_artifacts(run.info.run_id, local_artifacts_path, artifact_path=remote_artifacts_path)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T22:06:09.330741Z","iopub.execute_input":"2023-05-16T22:06:09.331107Z","iopub.status.idle":"2023-05-16T22:06:11.177403Z","shell.execute_reply.started":"2023-05-16T22:06:09.331080Z","shell.execute_reply":"2023-05-16T22:06:11.176380Z"},"trusted":true},"execution_count":null,"outputs":[]}]}